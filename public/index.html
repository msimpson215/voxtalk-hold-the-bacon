<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>VoxTalk</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body {
      margin:0; font-family:system-ui,sans-serif;
      display:grid; place-items:center; min-height:100vh;
      background: radial-gradient(circle at 50% 20%, #dbeafe, #93c5fd 40%, #1e3a8a 90%);
    }
    .app { text-align:center; max-width:600px; }
    h1 { margin:10px 0; font-size:22px; }
    #pttBtn {
      width:120px; height:120px; border-radius:50%; border:none; cursor:pointer;
      background: radial-gradient(circle at 30% 30%, #3b82f6, #1e40af);
      box-shadow:0 6px 18px rgba(37,99,235,.3);
    }
    #answer {
      margin-top:20px; padding:12px; border:1px solid #ccc;
      border-radius:8px; background:white; min-height:120px;
      font-size:14px; text-align:left; overflow-y:auto;
    }
    .line { margin:6px 0; }
    .me { color:#2563eb; font-weight:600; }
    .ai { color:#065f46; font-weight:600; }
    .text { color:#111; }
  </style>
</head>
<body>
  <div class="app">
    <h1>Talk to VoxTalk</h1>
    <button id="pttBtn">ðŸŽ¤</button>
    <div id="answer"><div class="muted">Conversation will appear here.</div></div>
    <audio id="remote" autoplay playsinline></audio>
  </div>

  <script>
    const pttBtn   = document.getElementById('pttBtn');
    const answerEl = document.getElementById('answer');
    const rtAudio  = document.getElementById('remote');

    function appendLine(role, text) {
      if (answerEl.querySelector('.muted')) answerEl.innerHTML = '';
      const div = document.createElement('div');
      div.className = 'line';
      div.innerHTML = `<span class="${role}">${role==='me'?'You:':'AI:'}</span>
                       <span class="text">${text}</span>`;
      answerEl.appendChild(div);
      answerEl.scrollTop = answerEl.scrollHeight;
    }

    async function initRealtime() {
      const s = await fetch("/session", { method:"POST" });
      const { client_secret, model, voice, deepgramKey } = await s.json();

      // --- OpenAI Realtime setup ---
      const pc = new RTCPeerConnection();
      pc.ontrack = (ev)=> { rtAudio.srcObject = ev.streams[0]; };

      const dc = pc.createDataChannel("events");
      dc.onopen = () => {
        dc.send(JSON.stringify({
          type:"session.update",
          session:{
            instructions:"Always respond in English with spoken audio only. Never switch languages.",
            voice:"verse",
            modalities:["audio"]
          }
        }));
      };
      dc.onmessage = (e)=> {
        try {
          const evt = JSON.parse(e.data);
          if (evt.type === "response.message.delta") {
            const chunk = evt.delta.map(d=>d.content?.[0]?.text||"").join("");
            if (chunk) appendLine("ai", chunk);
          }
        } catch {}
      };

      const offer = await pc.createOffer({ offerToReceiveAudio:true });
      await pc.setLocalDescription(offer);
      const r = await fetch(
        `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}&voice=${voice}`,
        {
          method:"POST",
          headers:{ "Authorization":`Bearer ${client_secret.value}`, "Content-Type":"application/sdp" },
          body: offer.sdp
        }
      );
      const answer = { type:"answer", sdp: await r.text() };
      await pc.setRemoteDescription(answer);

      // --- Mic capture ---
      const stream = await navigator.mediaDevices.getUserMedia({ audio:true });
      const micTrack = stream.getTracks()[0];
      pc.addTrack(micTrack, stream);

      // Deepgram uses the SAME mic track
      const dgStream = new MediaStream([micTrack]);

      // Force Opus codec with fallback
      let mime;
      if (MediaRecorder.isTypeSupported("audio/webm;codecs=opus")) {
        mime = "audio/webm;codecs=opus";
      } else if (MediaRecorder.isTypeSupported("audio/ogg;codecs=opus")) {
        mime = "audio/ogg;codecs=opus";
      } else {
        alert("âŒ No supported Opus codec in this browser.");
        return;
      }
      console.log("ðŸŽ™ï¸ Using mime:", mime);

      const dgSocket = new WebSocket(
        "wss://api.deepgram.com/v1/listen?model=nova&language=en",
        ["token", deepgramKey]
      );

      dgSocket.onopen = () => console.log("âœ… Deepgram socket open");
      dgSocket.onmessage = (msg) => {
        console.log("ðŸ“© Deepgram raw:", msg.data);
        try {
          const data = JSON.parse(msg.data);
          const transcript = data.channel?.alternatives?.[0]?.transcript;
          if (transcript && transcript.length > 0) {
            console.log("ðŸŽ¤ Transcript:", transcript);
            appendLine("me", transcript);
            // Send to OpenAI
            dc.send(JSON.stringify({
              type:"response.create",
              response:{ instructions: transcript }
            }));
          }
        } catch {}
      };

      // Recorder on SAME mic track
      const mediaRecorder = new MediaRecorder(dgStream, { mimeType: mime });
      mediaRecorder.ondataavailable = (e) => {
        console.log("ðŸŽ™ï¸ Blob size:", e.data.size);
        if (e.data.size > 0 && dgSocket.readyState === WebSocket.OPEN) {
          dgSocket.send(e.data);
        }
      };
      mediaRecorder.start(250);

      // Button just logs state
      let talking = false;
      pttBtn.onclick = () => {
        talking = !talking;
        appendLine("me", talking ? "(Listeningâ€¦)" : "(Stopped)");
      };
    }

    initRealtime();
  </script>
</body>
</html>
